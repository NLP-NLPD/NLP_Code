{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhMCta6KU4Yv"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/UpstageAI/cookbook/blob/main/cookbooks/upstage/Solar-Full-Stack LLM-101/05_3_OracleDB.ipynb\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TG6F1kAYU4Yx"
   },
   "source": [
    "# Try17: \n",
    "1. KB 구축\n",
    "- parsing: Upstage 모델, html 태그가 나와서 카테고리별로 자름. (페이지별 아님)\n",
    "- KB 추가: csv로 추가할 수 있음\n",
    "- 임베딩: Upstage 모델, FAISS\n",
    "2. 모델링\n",
    "- Upstage 모델\n",
    "3. 답변 생성\n",
    "- 질문에서 core_question 뽑고 pdf llm으로 찾기\n",
    "- N/A 나오면 core_question을 wiki로 검색하게 함. 이때 검색 언어는 core_question의 언어를 감지하게 하고, 검색은 단어 하나씩 검색하게 함. N/A 안 나올 때까지 반복.\n",
    "4. 자잘한 것들 추가\n",
    "- 답변 생성 시 맞게 검색해놓고 답안지를 선택하지 않을 경우가 있음: 답변은 선택지에서 선택하도록 프롬프팅\n",
    "- acc를 위해 비교 시 답안 추출이 안 되는 경우가 있음: normalize_answer 해서 비교하게 함\n",
    "- N/A가 한 번이 아니라 여러 번 나오는 경우가 있음: 안 나올 때까지 반복하게 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from openai import OpenAI # openai==1.52.2\n",
    "\n",
    "api_key = \"up_zsOzpjQ8Ow7NFmiWQPTh2x7P4Y4MQ\"\n",
    "data_path = \"/Users/susie/Desktop/Temp_Laptop2/Python_Files/G/24-2/NLP/Team/baseline/\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"up_zsOzpjQ8Ow7NFmiWQPTh2x7P4Y4MQ\",\n",
    "    base_url=\"https://api.upstage.ai/v1/solar\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = os.path.join(data_path, 'ewha.pdf')\n",
    "CSV_PATH = os.path.join(data_path, 'TestSamples.csv')\n",
    "# TABLE_PAGES 필요없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.schema import Document\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_text_or_table(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract and categorize text from a PDF using Upstage API.\n",
    "    \"\"\"\n",
    "    api_key = \"up_zsOzpjQ8Ow7NFmiWQPTh2x7P4Y4MQ\"\n",
    "    url = \"https://api.upstage.ai/v1/document-ai/document-parse\"\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    documents = []\n",
    "\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        response = requests.post(url, headers=headers, files={\"document\": file})\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        html_content = data.get(\"content\", {}).get(\"html\", \"\")\n",
    "        if not html_content:\n",
    "            print(\"Error: No HTML content found in API response.\")\n",
    "            return []\n",
    "\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        categories = {\n",
    "            \"table\": \"table\",\n",
    "            \"figure\": \"figure\",\n",
    "            \"chart\": \"img[data-category='chart']\",\n",
    "            \"heading1\": \"h1\",\n",
    "            \"header\": \"header\",\n",
    "            \"footer\": \"footer\",\n",
    "            \"caption\": \"caption\",\n",
    "            \"paragraph\": \"p[data-category='paragraph']\",\n",
    "            \"equation\": \"p[data-category='equation']\",\n",
    "            \"list\": \"p[data-category='list']\",\n",
    "            \"index\": \"p[data-category='index']\",\n",
    "            \"footnote\": \"p[data-category='footnote']\"\n",
    "        }\n",
    "\n",
    "        for category, selector in categories.items():\n",
    "            elements = soup.select(selector)\n",
    "            for element in elements:\n",
    "                content = element.get_text(strip=True)\n",
    "                metadata = {\"category\": category, \"html\": str(element)}\n",
    "                documents.append(Document(page_content=content, metadata=metadata))\n",
    "\n",
    "        if not documents:\n",
    "            print(\"No sections were extracted.\")\n",
    "        return documents\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return []\n",
    "    \n",
    "import re\n",
    "\n",
    "def clean_extracted_text(text):\n",
    "    # 문장 중간의 줄바꿈 제거\n",
    "    cleaned_text = re.sub(r'(?<=[a-z,])\\n(?=[a-z])', ' ', text)\n",
    "    # 문장 끝 줄바꿈 유지\n",
    "    cleaned_text = re.sub(r'(?<=[.?!])\\s*\\n', '\\n', cleaned_text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "import csv\n",
    "\n",
    "def load_problem_data(csv_path):\n",
    "    problems = []\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            question = row[0]\n",
    "            choices = row[1:-1]\n",
    "            answer = row[-1]\n",
    "            formatted_problem = f\"{question}\\n(A) {choices[0]}\\n(B) {choices[1]}\\n(C) {choices[2]}\\n(D) {choices[3]}\"\n",
    "            problems.append({\"type\": \"problem\", \"content\": formatted_problem, \"answer\": answer})\n",
    "    return problems\n",
    "\n",
    "\n",
    "def combine_kb(documents, problems):\n",
    "    combined_kb = [{\"type\": \"pdf\", \"content\": doc} for doc in documents]\n",
    "    combined_kb += [{\"type\": \"problem\", \"content\": problem} for problem in problems]\n",
    "    return combined_kb\n",
    "\n",
    "def ensure_text_format(kb):\n",
    "    \"\"\"\n",
    "    Convert a KB (list of dictionaries or strings) into a list of lists,\n",
    "    where each dictionary or string forms its own list of strings.\n",
    "    \"\"\"\n",
    "    if isinstance(kb, list):\n",
    "        processed_kb = []\n",
    "        for item in kb:\n",
    "            if isinstance(item, dict):\n",
    "                if \"content\" in item:\n",
    "                    content = item[\"content\"]\n",
    "                    if isinstance(content, dict) and \"page_content\" in content:\n",
    "                        processed_kb.append([content[\"page_content\"]])\n",
    "                    elif isinstance(content, str):\n",
    "                        processed_kb.append([content])\n",
    "            elif isinstance(item, str):\n",
    "                processed_kb.append([item])\n",
    "        return processed_kb\n",
    "    else:\n",
    "        raise ValueError(\"KB should be a list.\")\n",
    "\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "def split_text(text, max_length=1000):\n",
    "    \"\"\"\n",
    "    Split long text into manageable chunks.\n",
    "    \"\"\"\n",
    "    return [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
    "\n",
    "def preprocess_texts(texts):\n",
    "    \"\"\"\n",
    "    Preprocess input texts to ensure compatibility with the embeddings API.\n",
    "    \"\"\"\n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        if len(text.strip()) > 0:\n",
    "            cleaned_texts.append(text.strip())\n",
    "    return cleaned_texts\n",
    "\n",
    "class UpstageEmbeddings:\n",
    "    def __init__(self, client, model=\"embedding-query\"):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts.\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            try:\n",
    "                response = self.client.embeddings.create(\n",
    "                    model=self.model,\n",
    "                    input=text\n",
    "                )\n",
    "                embeddings.append(response.data[0].embedding)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating embedding for text: {text[:50]}... \\n{e}\")\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        \"\"\"\n",
    "        Generate an embedding for a single query.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.client.embeddings.create(\n",
    "                model=self.model,\n",
    "                input=text\n",
    "            )\n",
    "            return response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embedding for query: {text[:50]}... \\n{e}\")\n",
    "            return None\n",
    "\n",
    "def create_vector_store(kb_list):\n",
    "    documents = []\n",
    "    for entry in kb_list:\n",
    "        for text in entry:\n",
    "            processed_texts = preprocess_texts([text])\n",
    "            for processed_text in processed_texts:\n",
    "                chunks = split_text(processed_text, max_length=1000)\n",
    "                documents.extend([Document(page_content=chunk) for chunk in chunks])\n",
    "\n",
    "    embeddings = UpstageEmbeddings(client=client)\n",
    "\n",
    "    vector_store = FAISS.from_documents(documents, embeddings)\n",
    "    return vector_store\n",
    "\n",
    "def retrieve_context(question, vector_store):\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant context for a given question from the vector store.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        question_embedding = client.embeddings.create(\n",
    "            model=\"embedding-query\",\n",
    "            input=question\n",
    "        ).data[0].embedding\n",
    "\n",
    "        results = vector_store.similarity_search_by_vector(question_embedding, k=3)\n",
    "        return \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "    except Exception as e:\n",
    "        print(f\"Error during retrieval: {e}\")\n",
    "        return \"No relevant context found.\"\n",
    "    \n",
    "from openai import OpenAI\n",
    "\n",
    "def create_qa_chain(vector_store, model, prompt_template):\n",
    "    client = OpenAI(\n",
    "        api_key=\"up_zsOzpjQ8Ow7NFmiWQPTh2x7P4Y4MQ\",\n",
    "        base_url=\"https://api.upstage.ai/v1/solar\"\n",
    "    )\n",
    "\n",
    "    retriever = {\"embedding\": vector_store}\n",
    "    \n",
    "    def qa_model(prompt):\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    return {\"llm\": qa_model, \"retriever\": retriever, \"prompt\": prompt_template}\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def read_data(csv_path):\n",
    "    \"\"\"\n",
    "    input: sample.csv\n",
    "    output: prompts, answers\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(csv_path)\n",
    "    return data['prompts'].tolist(), data['answers'].tolist()\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_answer(text):\n",
    "    match = re.search(r\"\\([A-Z]\\)\", text)\n",
    "    return match.group(0) if match else \"N/A\"\n",
    "\n",
    "import wikipediaapi\n",
    "import re\n",
    "\n",
    "# Initialize Wikipedia APIs\n",
    "wiki_wiki_ko = wikipediaapi.Wikipedia(language='ko', user_agent='NLP_Team/1.0 (ewhanthbeot@ewhain.net)')\n",
    "wiki_wiki_en = wikipediaapi.Wikipedia(language='en', user_agent='NLP_Team/1.0 (ewhanthbeot@ewhain.net)')\n",
    "\n",
    "def detect_language(text):\n",
    "    if re.search(r'[가-힣]', text):\n",
    "        return 'ko'\n",
    "    elif re.search(r'[a-zA-Z]', text):\n",
    "        return 'en'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "def extract_core_question(question, core_extraction_model):\n",
    "    language = detect_language(question)\n",
    "    if language == 'ko':\n",
    "        prompt = f\"다음 질문에서 핵심 내용을 추출하세요 (선택지 제외):\\n\\n{question}\\n\\n핵심 내용:\"\n",
    "    elif language == 'en':\n",
    "        prompt = f\"Extract the core part of the following question (exclude choices):\\n\\n{question}\\n\\nCore content:\"\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported language detected.\")\n",
    "    core = core_extraction_model(prompt)\n",
    "    return core.strip()\n",
    "\n",
    "def fetch_from_wikipedia(query, language):\n",
    "    \"\"\"\n",
    "    Fetch a summary from Wikipedia based on the query and language.\n",
    "    \"\"\"\n",
    "    wiki_api = wiki_wiki_ko if language == 'ko' else wiki_wiki_en\n",
    "    page = wiki_api.page(query)\n",
    "    if page.exists():\n",
    "        return page.summary[:500]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def normalize_answer(answer):\n",
    "    \"\"\"\n",
    "    Normalize the format of the answer to handle cases like 'A' vs '(A)'.\n",
    "    \"\"\"\n",
    "    if answer.startswith(\"(\") and answer.endswith(\")\"):\n",
    "        return answer[1:-1]\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1. KB 구축\n",
    "# step 1.1 ewha.pdf에서 pdf parsing\n",
    "documents = extract_text_or_table(PDF_PATH)\n",
    "\n",
    "# step 1.2 pdf parsing한 것을 cleaning text\n",
    "# 각 페이지의 텍스트에 적용, 새로운 변수를 만들지 않고 이런 식으로 처리하기!!!\n",
    "cleaned_documents = []\n",
    "\n",
    "for doc in documents:\n",
    "    if isinstance(doc.page_content, str):\n",
    "        cleaned_content = clean_extracted_text(doc.page_content)\n",
    "        cleaned_documents.append({\"page_content\": cleaned_content, \"metadata\": doc.metadata})\n",
    "\n",
    "# CSV 문제 데이터 로드\n",
    "problems = \"problems.csv\"\n",
    "problems = load_problem_data(problems)\n",
    "\n",
    "# PDF 데이터와 문제 데이터 통합\n",
    "kb = combine_kb(cleaned_documents, problems)\n",
    "\n",
    "# 통합 후 리스트로 변환\n",
    "list_kb = ensure_text_format(kb)\n",
    "\n",
    "# step 1.4 전체 KB 임베딩하고 <Embedding 모델> 검색할 수 있게 <VectorStore 모델> 함 - 토큰 너무 많아지는 것을 방지하기 위해 split\n",
    "vector_store = create_vector_store(list_kb)\n",
    "\n",
    "# STEP 2. 모델링 <llm 모델> <QA 방식> - 언어 모델, 모델링 방식, 프롬프팅 준비\n",
    "model = \"solar-pro\"\n",
    "custom_prompt = \"\"\"\n",
    "다음 문서에서 이유를 찾고 질문에 답하세요: \\n\n",
    "{context}\\n\n",
    "1. Tables in the document follow a hierarchical structure:\n",
    "- **Example**: \n",
    "    ```\n",
    "    [['대학', '학부/학과/전공', '입학정원'], \n",
    "    ['사범대학', '교육학과\\n유아교육과...', '27\\n29...']]\n",
    "    ```\n",
    "- This indicates:\n",
    "    - A university (e.g., 사범대학) contains colleges (e.g., 교육학과).\n",
    "    - A college can further have departments or majors (e.g., 특수교육과 includes 유아특수교육전공).\n",
    "\\n\n",
    "질문: {question}\\n\n",
    "답변은 선택지에서 선택하세요.\"\"\"\n",
    "qa_chain = create_qa_chain(vector_store, model, custom_prompt)\n",
    "\n",
    "# STEP 3. samples에서 질문과 정답 가져오기 - csv 파일에서 -> list로 변경\n",
    "prompts, answers = read_data(CSV_PATH)\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for i, question in enumerate(prompts):\n",
    "    core_question = extract_core_question(question, qa_chain[\"llm\"])\n",
    "    print(f\"핵심 질문 추출: {core_question}\")\n",
    "    \n",
    "    context = retrieve_context(core_question, vector_store)\n",
    "    \n",
    "    if not context.strip():\n",
    "        print(\"PDF에서 관련 정보를 찾을 수 없습니다. Wikipedia를 검색합니다.\")\n",
    "        words = re.findall(r'[가-힣]+|[a-zA-Z]+|\\d+', core_question)\n",
    "        language = detect_language(core_question)\n",
    "        \n",
    "        for word in words:\n",
    "            context = fetch_from_wikipedia(word, language)\n",
    "            if context:\n",
    "                print(f\"Wikipedia에서 관련 정보를 찾았습니다: {word}\")\n",
    "                break  \n",
    "        else:\n",
    "            context = \"No relevant information found.\"\n",
    "\n",
    "    words = re.findall(r'[가-힣]+|[a-zA-Z]+|\\d+', core_question)\n",
    "    language = detect_language(core_question) \n",
    "    while True: \n",
    "        prompt = qa_chain[\"prompt\"].format(context=context, question=question)\n",
    "        result = qa_chain[\"llm\"](prompt)\n",
    "        predicted_answer = extract_answer(result)\n",
    "        \n",
    "        if predicted_answer != \"N/A\":\n",
    "            break\n",
    "        \n",
    "        print(f\"답변이 N/A로 표시됨: 다시 확인 중...\")\n",
    "        \n",
    "        if words:\n",
    "            word = words.pop(0)\n",
    "            context = fetch_from_wikipedia(word, language)\n",
    "            if context:\n",
    "                print(f\"Wikipedia에서 관련 정보를 찾았습니다: {word}\")\n",
    "        else:\n",
    "            context = \"No relevant information found.\"\n",
    "            break\n",
    "\n",
    "    normalized_predicted = normalize_answer(predicted_answer)\n",
    "    normalized_actual = normalize_answer(answers[i])\n",
    "\n",
    "    print(f\"질문 {i + 1}: {question}\")\n",
    "    print(f\"생성된 답변: {result}\")\n",
    "    print(f\"예상 답: {normalized_predicted}, 실제 답: {normalized_actual}\\n\")\n",
    "\n",
    "    if normalized_predicted == normalized_actual:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"총 {len(prompts)}개의 질문 중 {correct}개 맞춤. 정확도: {correct / len(prompts) * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
